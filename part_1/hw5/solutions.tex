\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}

\begin{document}

\title{Solutions to hw5 homework on Convex Optimization https://web.stanford.edu/class/ee364a/homework.html}
\author{Andrei Keino}
\maketitle

% https://web.stanford.edu/class/ee364a/homework.html
% 6.3, 6.8a-b, A5.4, A12.6, A14.8, A17.9

% 5.17 - p. 278
% Robust linear programming cvxbook p. 157
% duality - from p.215

\section*{5.17}
Robust linear programming with polyhedral uncertainty. Consider the robust LP:

\begin{align*}
&\text{minimize } && c^T x \\
&\text{subject to } &&\sup_{a \in P_i} a^T x \leq b_i, \; i = 1, ... , m\\
\end{align*} 

with variable $x \in R^n, $ where
$P_i = \{a: \; C_i a \preceq d_i \}.$ The problem data are $a \in R^n,$ $C_i \in R^{m_i \times n}, $ $d_i \in R^{m_i}, $ and $b \in R^m.$ We asume the polyhedra $P_i$ are nonempty. Show that this problem is equivalent to the LP:

\begin{align*}
&\text{minimize } && c^T x \\
&\text{subject to } &&d_i^T z_i \leq b_i,
\; i = 1, ... , m\\
& && C_i z_i = x, \; i = 1, ... , m\\
& && z_i \succeq 0, \; i = 1, ... , m\\
\end{align*} 
with variables $x \in R^n, $ 
$z_i \in R^{m_i}, \; i = 1, ... , m.$ 
Hint: find the dual of the problem of maximizing 
$a_i^T x$ over $a_i \in P_i$ (with variable $a_i$).\\

Solution: \\

The problem of maximizing 
$a_i^T x$ over $a_i \in P_i$ (with variable $a_i$) is:

\begin{align*}
&\text{maximize } && a_i^T x \\
&\text{subject to } &&a_i \in P_i, \text{where } 
P_i = \{a: \; C_i a \preceq d_i \}\\
\end{align*} 

or 

\begin{align*}
&\text{minimize } && - a_i^T x \\
&\text{subject to } &&C_i a_i \preceq d_i \\
\end{align*} 

The Lagrange dual of this problem is:

\begin{align*}
&\text{minimize } && \sum_{i = 1}^{m} \lambda_i d_i \\
&\text{subject to } 
&& C_i \lambda_i = x \\
& &&\lambda_i \succeq 0\\
\end{align*} 

The optimal value of this problem is less or equal to $b_i$, so we have the equivalent problem to our LP:

\begin{align*}
&\text{minimize } && c^T x \\
&\text{subject to } &&d_i^T \lambda_i \leq b_i,
\; i = 1, ... , m\\
& && C_i \lambda_i = x, \; i = 1, ... , m\\
& && \lambda_i \succeq 0, \; i = 1, ... , m\\
\end{align*} 

\section*{5.40}
% ex. 5.40 - p. 286
% ex. 5.10 - p. 276
% paragraph 7.5 - p. 384
 
E - optimal experiment design. A variation on two optimal experiment design problems of exercise 5.10
is the E - optimal design problem:

\begin{align*}
&\text{minimize } && \lambda_{max}
(\sum_{i = 1}^p x_i v_i v_i^T)^{-1} \\
&\text{subject to } 
&&x \succeq 0, \; \boldsymbol{1}^T x = 1\\
\end{align*} 

(See also \S 7.5.) Derive a dual for this problem first by reformulating it as: 
 
\begin{align*}
&\text{minimize } && 1 / t \\
&\text{subject to } 
&& \sum_{i = 1}^p x_i v_i v_i^T
\succeq t \boldsymbol{I} \\
& &&x \succeq 0, \; \boldsymbol{1}^T x = 1
\end{align*} 

with variables $t \in R,$ $x \in R^p$ and domain 
$R_{++} \times R^p,$ and applying Lagrange duality. Simplify the dual problem as much as you can.

Solution:
Let us introduce a variable $t  \in R_{++}.$ Then for a matrix $A,$ inequality 
$\lambda_{max}(A^{-1})  \leq 1 / t$ means $A^{-1} \preceq \frac{1}{t}\boldmath{I},$ or 
$A \preceq t \boldmath{I}.$ Setting $A$ to 
$\sum_{i = 1}^p x_i v_i v_i^T$ we get a problem:

\begin{align*}
&\text{minimize } && 1 / t \\
&\text{subject to } 
&& \sum_{i = 1}^p x_i v_i v_i^T
\succeq t \boldsymbol{I} \\
& &&x \succeq 0, \; \boldsymbol{1}^T x = 1
\end{align*} 

% Entropy maximization - p.222 
% Generalized inequalities - p. 264
% Example 5.11 Lagrange dual of semidefinite program - p. 265

% Lagrange function - https://math.stackexchange.com/questions/462896/is-positive-semidefinite-matrix-same-as-positive-number-in-convex-optimisation

% https://en.wikipedia.org/wiki/Semidefinite_programming

% Semidefinite programming - p. 168

The Lagrangian is: 

$$
L(x, t, Z, z) = 1 / t - 
tr(Z (\sum_{i = 1}^p x_i v_i v_i^T - t \boldsymbol{I})) - z^T x + 
\nu (\boldsymbol{1}^T x - 1) \\ 
= 1 / t + t \, tr(Z) + 
\sum_{i = 1}^p x_i 
(-v_i^T Z v_i - z_i + \nu) - \nu
$$
the infimum of x is bounded below only if 
$-v_i^T Z v_i - z_i + \nu = 0.$ The 

$$
inf_\nu 1/t  + t \, tr(Z) = 
\begin{cases}
2 \sqrt{tr(Z)}, & Z \succeq 0 \\
- \infty, & \text{otherwise}
\end{cases}
$$
the dual function is 

$$
L(Z, z, \nu) = 
\begin{cases}
2 \sqrt{tr(Z)} - \nu, & Z \succeq 0, \,  
-v_i^T Z v_i - z_i + \nu = 0,\\
- \infty, & \text{otherwise}
\end{cases}
$$

The dual problem:

\begin{align*}
&\text{maximize } && 2\sqrt{tr(Z)} - \nu \\
&\text{subject to } 
&& v_i^T Z v_i + z_i \leq 0, 
\; i = 1, ..., p  \\
& &&Z \succeq 0, \, \nu \geq 0\\
\end{align*} 

We can define $W = (1 / \nu)Z$
\begin{align*}
&\text{maximize } && 2\sqrt{\nu}\sqrt{tr(W)} - \nu \\
&\text{subject to } 
&& v_i^T W v_i \leq 1, 
\; i = 1, ..., p  \\
& &&W \succeq 0, \, \nu \geq 0
\end{align*} 

Maximizing over $\nu$ we get $\nu = tr(W), $
so the problem is:

\begin{align*}
&\text{maximize } && tr(W) \\
&\text{subject to } 
&& v_i^T W v_i \leq 1, 
\; i = 1, ..., p  \\
& &&W \succeq 0\\
\end{align*} 

\section*{6.3}
% p. 344

%LPs, QPs, SOCPs, or SDPs

% LP - Linear optimization problems - p. 146
% QP - Quadratic optimization problems - p.152

% Geometric programming - p. 160
% SDP - Semidefinite programming - p. 168
% SOCP - Second-order cone programming - p. 156
% QCQP - p. 153
% GP geometric program - p. 161

Formulate the following approximation problems as LPs, QPs, SOCPs, or SDPs. The
problem data are $A \in R^{n \times m}$ and 
$b \in R^m.$ The rows of $A$ are denoted $a_i^T.$

(a) Deadzone-linear penalty approximation: minimize $\sum_{i = 1}^m \phi(a_i^T x - b_i),$ where 

$$
\phi(u) = 
\begin{cases}
0, & |u| \leq a \\
|u| - a, & |u| > a
\end{cases}
$$
where $a > 0.$

(b) Log-barrier penalty approximation: 
minimize $\sum_{i = 1}^m \phi(a_i^T x - b_i),$ where 

$$
\phi(u) = 
\begin{cases}
- a^2 log(1 - (u / a)^2), & |u| < a \\
\infty, & |u| \geq a
\end{cases}
$$
with $a > 0.$

(c) Huber penalty approximation: minimize 
 $\sum_{i = 1}^m \phi(a_i^T x - b_i),$ where 

$$
\phi(u) = 
\begin{cases}
u^2, & |u| \leq M \\
M(2|u| - M), & |u| > M
\end{cases}
$$
with $M > 0.$

(d) Log-Chebyshev approximation: minimize
$max_{i = 1, ..., m} 
|log(a_i^T x) - log(b_i)|.$ We assume 
$b \succ 0.$ An equivalent convex from is

\begin{align*}
&\text{minimize } && t \\
&\text{subject to } 
&& 1/t \leq a_i^Tx/b_i \leq t, \, i = 1 ,..., m\\
\end{align*} 
with variables $x \in R^n$ and $t \in R$ and domain $R^n \times R_{++}.$

(e) Minimizing the sum of the largest k residuals:

\begin{align*}
&\text{minimize } && 
\sum_{i = 1}^k |r|_{[i]} \\
&\text{subject to } 
&& r = Ax - b\\
\end{align*} 

where $|r|_{[1]} \geq |r|_{[2]} \geq ,..., 
\geq |r|_{[m]}$ are the numbers 
$|r_1|, |r_2|, ... , |r_m| $ sorted in decreasing order. (For $k = 1$ this reduces to $l_\infty$ - norm approximation; for $k = m$ this reduces to $l_1$ norm approximation.) Hint: See exercise 5.19.

Solution:

(a) Introduce the variable $y \in R^m$ so that 
$|a_i^T x - b_i| \leq |a + y_i|, \, y_i \geq 0$

then we have equivalent problem:
\begin{align*}
&\text{minimize } && \boldsymbol{1}^Ty \\
&\text{subject to } 
&& - y - a \boldsymbol{1} \preceq A x - b 
\preceq y + a \boldsymbol{1} \\
& && y \ \succeq 0\\
\end{align*}

(b)

Introduce the variable $y = Ax - b, $ 
$t_i \leq (1 - y_i / a)(1 + y_i + a)$, and add inequality $-a \leq y_i \leq a$ this problem will transform to:

\begin{align*}
&\text{maximize } && \prod_{i = 1}^m t_i^2 \\
&\text{subject to } && y = A x - b \\
& && t_i \leq (1 - y_i / a)(1 + y_i / a),
\, i = 1, ..., m\\
& && -1 \leq y_i / a \leq 1, \, i = 1, ..., m\\
\end{align*}
with variables $t, y \in R^m,$ $x \in R^n.$
Next assume that $m = 2^k.$ It can be done without loss of generality, as in other case we always can add missing components of $y_i$ as unity values ($a_i = 0$ and 
$b_i = - 1$). Lets transform this problem for $m = 4.$

\begin{align*}
&\text{maximize } && (t_1 t_2 t_3 t_4)^2 \\
&\text{subject to } && y = A x - b \\
& && t_i \leq (1 - y_i / a)(1 + y_i / a),
\, i = 1, ..., m\\
& && -1 \leq y_i / a \leq 1, \, i = 1, ..., m\\
\end{align*}

this problem is equivalent to:

\begin{align*}
&\text{maximize } && z_1 z_2 \\
&\text{subject to } && \\
& && z_1^2 \leq t_1 t_2 \\
& && z_2^2 \leq t_3 t_4 \\
& && y = A x - b \\
& && t_i \leq (1 - y_i / a)(1 + y_i / a),
\, i = 1, ..., m\\
& && -1 \leq y_i / a \leq 1, \, i = 1, ..., m\\
\end{align*}

and also as:

\begin{align*}
&\text{maximize } && z \\
&\text{subject to } && \\
& && z^2 \leq z_1 z_2\\
& && z_1^2 \leq t_1 t_2 \\
& && z_2^2 \leq t_3 t_4 \\
& && y = A x - b \\
& && t_i \leq (1 - y_i / a)(1 + y_i / a),
\, i = 1, ..., m\\
& && -1 \leq y_i / a \leq 1, \, i = 1, ..., m\\
\end{align*}

as it is easy to show that $x^T x \leq yz$ where 
$x \in R^n, $ $y, z \in R_+$ is equivalent to:

$$
\left\lVert
\begin{bmatrix}
	x \\ y - z\\
\end{bmatrix}
\right\rVert_2 \leq y + z
$$

overriding the first three inequalities with their norm analogue we have:

\begin{align*}
&\text{minimize } && - z \\
&\text{subject to } && \\
& && 
\left\lVert
\begin{bmatrix}
z \\ z_1 - z_2\\
\end{bmatrix}
\right\rVert_2 \leq z_1 + z_2\\
& && \left\lVert
\begin{bmatrix}
z_1 \\ t_1 - t_2\\
\end{bmatrix}
\right\rVert_2 \leq t_1 + t_2\\
& && \left\lVert
\begin{bmatrix}
z_2 \\ t_3 - t_4\\
\end{bmatrix}
\right\rVert_2 \leq t_3 + t_4\\
& && y = A x - b \\
& && t_i \leq (1 - y_i / a)(1 + y_i / a),
\, i = 1, ..., m\\
& && -1 \leq y_i / a \leq 1, \, i = 1, ..., m\\
\end{align*}

which is Second Order Cone Program (SOCP). \\

(c)\\
Lets show that this problem is equivalent to QP:
\begin{align*}
&\text{minimize } && 
\sum_{i = 1}^m(u_i^2 + 2 M v_i) \\
&\text{subject to } && 
-u - v \preceq Ax - b \preceq u + v\\
& && 0 \preceq u \preceq M \boldsymbol{1}\\
& && v \succeq 0
\end{align*}
Proof: Lets fix $x$ in our QP. For the optimum point we must have \\
$u_i + v_i = |a_i^T x - b_i|.$
In other case, if 
$u_i + v_i > |a_i^T x b_i|$ and 
$0 \leq u_i \leq M$ and $v_i \geq 0, $ then as $u_i$ and $v_i$ are not both zero, we can decrease $u_i$ and/or $v_i$ without violating the constraints and the objective will be decreased also. So, at the optimum we have:
$$ v_i = |a_i^T x - b_i| - u_i$$ 
Eliminating $v_i$ yields equivalent problem:
\begin{align*}
&\text{minimize } && 
\sum_{i = 1}^m
(u_i^2 - 2 M u_i + 2 M |a_i^T x - b_i|) \\
&\text{subject to }
&& 0 \preceq u_i \preceq 
min(M, \, |a_i^T x - b_i|)\\
\end{align*}
It $M > |a_i^T x - b_i|$ the optimal choice for 
$u_i$ is $|a_i^T x - b_i|.$ In this case the objective function reduces to 
$|a_i^T x - b_i|^2.$ Otherwise the optimal choice for $u_i$ is $M,$ and the objective function reduces to $2 M |a_i^T x - b_i| - M^2.$ So, we conclude that with $\phi(a_i^T x - b_i)$ these problems are equivalent.\\
(c) The constraint $ta_i^Tx \geq b_i,$ $t \geq 0,$
$a_i^T x \geq 0$ can be formulated as an LMI 
$$
\begin{bmatrix}
	t & \sqrt{b_i}\\
	\sqrt{b_i} & a_i^Tx\\
\end{bmatrix} \succeq 0
$$

or as follows:

$$
\left\Vert
\begin{bmatrix}
 2\sqrt{b_i}\\
t - a_i^Tx\\
\end{bmatrix} 
\right\Vert_2 \leq t + a_i^Tx
$$

% 5.19 - p. 278
% this - p.345
(e) As in exercise 5.19, we have a problem:

\begin{align*}
&\text{minimize } && 
kt + \boldsymbol{1}z \\
&\text{subject to }
&& - t \boldsymbol{1} - z \preceq Ax - b \preceq 
t \boldsymbol{1} +z \\
& && z \succeq 0
\end{align*}
where $x \in R^n,$ $t \in R,$ $z \in R^m$

\section*{6.8 a - b} % p. 346
Formulate the following robust approximation problems as LPs, QPs, SOCPs, or SDPs. For each subproblem, consider the $l_1 - ,$  $l_2 - ,$ and the $l_{\infty} - $ norms.

(a) Stochastic robust approximation with a finite set of parameter values, i.e., the sum of - norms problem

\begin{align*}
&\text{minimize } && 
\sum_{i = 1}^k
p_i\left\lVert A_ix - b\right\lVert  \\
\end{align*}
where $p \succeq 0$ and $\boldsymbol{1}^Tp = 1.$ 
(See \S6.4.1.)

(b) Worst-case robust approximation with coefficient bounds:
\begin{align*}
&\text{minimize } && 
\sup_{A \in \mathcal{A}}
\left\lVert Ax - b\right\lVert  \\
\end{align*}
where $\mathcal{A} = \{A \in R^{m\times n} | 
l_{ij} \leq a_{ij} \leq u_{ij, i = 1, ..., m, 
	j = 1, ... , n}\}.$ Here the uncertainty set is described by giving upper and lower bounds for the
components of $A$. We assume $l_{ij} < u_{ij}.$

\end{document}

